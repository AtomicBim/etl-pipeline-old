{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3abbfb4",
   "metadata": {},
   "source": [
    "## Импорты, подключения и сбор датасетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce6f1d",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nfrom sqlalchemy import create_engine\nimport pathlib\nimport os\n\nBASE_DIR = pathlib.Path(os.environ['ETL_ROOT'])\nRAW_DIR = BASE_DIR / 'raw_data'\nPROC_DIR = BASE_DIR / 'processed'\n\nad_path                        = RAW_DIR / 'tim_export_ad_user.csv'\nsync_path                      = RAW_DIR / 'tim_export_project_sync.csv'\n\nsave_path = PROC_DIR / 'sync_transformed.csv'\n\nengine_postgres = create_engine(\"postgresql+psycopg2://postgres:Q!w2e3r4@192.168.42.188:5430/postgres\")\nengine_pluginsdb = create_engine(\"postgresql+psycopg2://postgres:Q!w2e3r4@192.168.42.188:5430/pluginsdb\")"
  },
  {
   "cell_type": "markdown",
   "id": "48ca2d7a",
   "metadata": {},
   "source": [
    "## Создание датафреймов по источникам данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efe1416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ad = pd.read_csv(ad_path)\n",
    "df_sync = pd.read_csv(sync_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37703e5",
   "metadata": {},
   "source": [
    "## Слияние с AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af3f8460",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sync = df_sync.merge(\n",
    "    df_ad[[\"display_name\", \"department\", \"project_section\"]],\n",
    "    how=\"left\",\n",
    "    left_on=\"user_display_name\",\n",
    "    right_on=\"display_name\"\n",
    ").drop(columns=\"display_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1225895f",
   "metadata": {},
   "source": [
    "## Создание коротких названий проектов и удаление лишних столбцов в мониторинге"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57f67118",
   "metadata": {},
   "outputs": [],
   "source": [
    "bim_users = {\n",
    "    'Колпаков Семен Дмитриевич','Пятков Роман Анатольевич',\n",
    "    'Андреев Александр Константинович','Кичигин Андрей Владимирович',\n",
    "    'Панов Антон Владимирович','Васьков Денис Игоревич','Попов Антон Михайлович',\n",
    "    'Кузовлева Ольга Сергеевна','Калачев Даниил Артемович',\n",
    "    'Григорьев Роман Николаевич','Красильников Дмитрий Сергеевич',\n",
    "    'Литуева Юлия Дмитриевна','Жук Виталий Томашевич','Овсянкин Роман Николаевич',\n",
    "    'Романова Анна Вячеславовна','Коновалов Василий Сергеевич',\n",
    "    'Урманчеев Роман Дамирович', 'Докладчик 708'\n",
    "}\n",
    "\n",
    "df_sync[\"is_bim\"] = df_sync[\"user_display_name\"].isin(bim_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8cce841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_short_name(name: str) -> str:\n",
    "    parts = name.split('_')\n",
    "    return '_'.join(parts[:2]) if len(parts) >= 2 else name\n",
    "\n",
    "df_sync['short_project_name'] = df_sync['project_name'].astype(str).apply(extract_short_name)\n",
    "\n",
    "df_sync = df_sync.drop(columns=[\n",
    "    'program_name',\n",
    "    'program_version',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26b6aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_atom = df_sync[\"project_name\"].str.contains(\n",
    "    \"АТОМ|ДОУ|08-12|ИКП|ATOM|АПУ\", case=False, na=False\n",
    ")\n",
    "\n",
    "df_sync[\"object_name\"] = np.select(\n",
    "    [\n",
    "        df_sync[\"project_name\"].str.contains(\"СП.ЛЛУ|стандарт|узлы|узел|библиотека\", case=False, na=False),\n",
    "        mask_atom,\n",
    "        df_sync[\"project_name\"].str.contains(\"K01\", case=False, na=False),\n",
    "        df_sync[\"project_name\"].str.contains(\"ИНПРО\", case=False, na=False),\n",
    "        df_sync[\"project_name\"].str.contains(\"Ялта\", case=False, na=False)\n",
    "    ],\n",
    "    [\n",
    "        \"Узлы и стандарты\",\n",
    "        \"АТОМ\",\n",
    "        \"Кортрос\",\n",
    "        \"ИНПРО\",\n",
    "        \"Ялта\"\n",
    "    ],\n",
    "    default=\"Неизвестные проекты\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cc9ebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sync[\"is_detached\"] = df_sync[\"project_name\"].str.contains(\"отсоединено\", case=False, na=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ce27465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file_storage_name(row):\n",
    "    project = row.get(\"project_name\")\n",
    "    username = row.get(\"username\")\n",
    "\n",
    "    if pd.isna(project) or pd.isna(username):\n",
    "        return project  # оставить как есть\n",
    "\n",
    "    parts = str(project).split(\"_\")\n",
    "    if len(parts) < 2:\n",
    "        return project  # оставить как есть\n",
    "\n",
    "    last_part = parts[-1].strip().lower()\n",
    "    user_name = str(username).strip().lower()\n",
    "\n",
    "    if last_part == user_name:\n",
    "        return \"_\".join(parts[:-1])\n",
    "    else:\n",
    "        return project  # оставить как есть\n",
    "\n",
    "df_sync[\"file_storage_name\"] = df_sync.apply(extract_file_storage_name, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c11cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_project_solution(row):\n",
    "    name = str(row[\"project_name\"])\n",
    "    obj  = row[\"object_name\"]\n",
    "\n",
    "    if obj == \"Кортрос\":\n",
    "        section_map_kortros = {\n",
    "            \"_AR\": \"АР\", \"_AI\": \"АИ\",\n",
    "            \"_KR\": \"КР\", \"_AGK\": \"АГК\",\n",
    "            \"_VK\": \"ВК\", \"_EL\": \"ЭЛ\",\n",
    "            \"_OV\": \"ОВ\", \"_AK\": \"АК\",\n",
    "            \"_SS\": \"СС\", \"_P\": \"П\",\n",
    "            \"_R\": \"Р\", \"_TS\": \"ТС\",\n",
    "            \"_AP\": \"АП\"\n",
    "        }\n",
    "\n",
    "        for pattern, section in section_map_kortros.items():\n",
    "            if pattern in name:\n",
    "                return section\n",
    "        return \"НД\"\n",
    "\n",
    "    else:\n",
    "        section_map_rus = {\n",
    "            \"_АР\": \"АР\", \"_Форэскиз\": \"АР\",\n",
    "            \"_АИ\": \"АИ\", \"_КЖ\": \"КЖ\",\n",
    "            \"_ВК\": \"ВК\", \"_ЭЛ\": \"ЭЛ\",\n",
    "            \"_ТС\": \"ТС\", \"_ТХ\": \"ТХ\",\n",
    "            \"_ОВ\": \"ОВ\", \"_КР\": \"КР\",\n",
    "            \"_КМ\": \"КМ\", \"_АП\": \"АП\",\n",
    "            \"_ПТ\": \"ПТ\", \"_СС\": \"СС\",\n",
    "            \"_ПБ\": \"ПБ\", \"_ЭГ\": \"ЭГ\",\n",
    "             \"_АП\": \"АП\"\n",
    "        }\n",
    "\n",
    "        for pattern, section in section_map_rus.items():\n",
    "            if pattern in name:\n",
    "                return section\n",
    "        return \"НД\"\n",
    "\n",
    "df_sync[\"project_solution_name\"] = df_sync.apply(get_project_solution, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b945d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_project_stage(row):\n",
    "    name = str(row[\"project_name\"])\n",
    "    obj = row[\"object_name\"]\n",
    "\n",
    "    if obj == \"Кортрос\":\n",
    "        # кортеж: (тип_проверки, паттерн): значение\n",
    "        stage_map_kortros = {\n",
    "            (\"contains\", \"_P_\"): \"П\",\n",
    "            (\"contains\", \"_R_\"): \"Р\",\n",
    "            (\"contains\", \"_AGK_\"): \"ГК\",\n",
    "            (\"endswith\", \"_P\"): \"П\",\n",
    "            (\"endswith\", \"_R\"): \"Р\",\n",
    "            (\"endswith\", \"_AGK\"): \"ГК\"\n",
    "        }\n",
    "\n",
    "        for (mode, pattern), stage in stage_map_kortros.items():\n",
    "            if (mode == \"contains\" and pattern in name) or \\\n",
    "               (mode == \"endswith\" and name.endswith(pattern)):\n",
    "                return stage\n",
    "\n",
    "        return \"НД\"\n",
    "\n",
    "    else:\n",
    "        stage_map = {\n",
    "            (\"contains\", \"_П_\"): \"П\",\n",
    "            (\"contains\", \"_Р_\"): \"Р\",\n",
    "            (\"contains\", \"_РД_\"): \"Р\",\n",
    "            (\"contains\", \"_ЭП_\"): \"ЭП\",\n",
    "            (\"contains\", \"_Форэскиз_\"): \"ЭП\",\n",
    "            (\"contains\", \"_Эскиз_\"): \"ЭП\",\n",
    "            (\"contains\", \"_ФЭ_\"): \"ЭП\",\n",
    "            (\"endswith\", \"_П\"): \"П\",\n",
    "            (\"endswith\", \"_Р\"): \"Р\",\n",
    "            (\"endswith\", \"_РД\"): \"Р\",\n",
    "            (\"endswith\", \"_ЭП\"): \"ЭП\",\n",
    "            (\"endswith\", \"_Форэскиз\"): \"ЭП\",\n",
    "            (\"endswith\", \"_Эскиз\"): \"ЭП\",\n",
    "            (\"endswith\", \"_ФЭ\"): \"ЭП\"\n",
    "        }\n",
    "\n",
    "        for (mode, pattern), stage in stage_map.items():\n",
    "            if (mode == \"contains\" and pattern in name) or \\\n",
    "               (mode == \"endswith\" and name.endswith(pattern)):\n",
    "                return stage\n",
    "\n",
    "        return \"НД\"\n",
    "    \n",
    "df_sync[\"project_stage_name\"] = df_sync.apply(get_project_stage, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04a683aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_cols = df_sync.select_dtypes(include='object').columns\n",
    "df_sync[str_cols] = df_sync[str_cols].fillna(\"Нет данных\")\n",
    "\n",
    "num_cols = df_sync.select_dtypes(include=['number', 'Int64']).columns\n",
    "df_sync[num_cols] = df_sync[num_cols].fillna(0)\n",
    "\n",
    "date_cols = df_sync.select_dtypes(include='datetime').columns\n",
    "df_sync[date_cols] = df_sync[date_cols].fillna(pd.NaT)  # Или заменить:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0a61cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sync_bim = df_sync[(df_sync['is_bim'] == True) & (df_sync['is_detached'] == 0)].copy()\n",
    "df_sync_designers = df_sync[(df_sync['is_bim'] == False) & (df_sync['is_detached'] == 0)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beec76a9",
   "metadata": {},
   "source": [
    "## Пишем в БД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddca4c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Подключен к базе: postgres\n",
      "🛠 Структура таблицы datalake.test_lake пересоздана из DataFrame.\n",
      "✅ Загружено строк: 240966\n",
      "🛠 Структура таблицы datalake.test_lake пересоздана из DataFrame.\n",
      "✅ Загружено строк: 16425\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Инкрементальная загрузка DataFrame‑ов в PostgreSQL.\n",
    "\n",
    "**Почему выбран ключ‑дата, а не GUID?**\n",
    "-----------------------------------------------------------------\n",
    "* В ваших выгрузках строки появляются *один раз* и больше не меняются, а новые — отличаются полем‑даты (snapshot / update_date / loaded_at).\n",
    "* Максимальная дата в таблице вычисляется за O(1) при наличии индекса и сразу говорит, какие строки новые.\n",
    "* GUID, сгенерированный на лету (`uuid4()`), не детерминирован: при следующем запуске старые строки получат другие GUID‑ы и будут вставлены повторно.\n",
    "* Дет‑GUID из хэша строки требует вычислений и всё‑равно дублирует функцию даты.\n",
    "\n",
    "Итого: **ключ‑дата проще, быстрее и не меняет схему данных**.\n",
    "\n",
    "----------------------------------------------------------------------------------------------------\n",
    "Скрипт\n",
    "====================================================================\n",
    "Зависимости: pandas, numpy, SQLAlchemy ≥1.4, psycopg2‑binary.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, inspect, text\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# Подключение\n",
    "# ─────────────────────────────────────────────\n",
    "SCHEMA = \"datalake\"\n",
    "CHUNK_SIZE = 5_000\n",
    "\n",
    "def _ensure_table_and_columns(df: pd.DataFrame, table: str, conn) -> None:\n",
    "    \"\"\"Создаёт таблицу или добавляет недостающие столбцы.\"\"\"\n",
    "    insp = inspect(conn)\n",
    "    if not insp.has_table(table, schema=SCHEMA):\n",
    "        df.head(0).to_sql(table, conn, schema=SCHEMA, if_exists=\"replace\", index=False)\n",
    "        return\n",
    "\n",
    "    existing_cols = {c[\"name\"] for c in insp.get_columns(table, schema=SCHEMA)}\n",
    "    missing_cols = [c for c in df.columns if c not in existing_cols]\n",
    "    for col in missing_cols:\n",
    "        sql_type = str(df[col].dtype)\n",
    "        # упрощённо: pandas приведёт к text/varchar; для строгой типизации можно\n",
    "        # вызвать pandas.io.sql.get_sqltype как в прошлой версии.\n",
    "        conn.execute(\n",
    "            text(\n",
    "                f'ALTER TABLE \"{SCHEMA}\".\"{table}\" '\n",
    "                f'ADD COLUMN IF NOT EXISTS \"{col}\" {sql_type}'\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def _incremental_append(\n",
    "    df: pd.DataFrame,\n",
    "    table: str,\n",
    "    date_col: str,\n",
    "    conn,\n",
    ") -> int:\n",
    "    \"\"\"Вставляет только строки, у которых `date_col` > max(date_col) в таблице.\"\"\"\n",
    "    _ensure_table_and_columns(df, table, conn)\n",
    "\n",
    "    # если дата‑колонки нет — ошибка пользователя\n",
    "    if date_col not in df.columns:\n",
    "        raise KeyError(f\"В DataFrame отсутствует колонка с датой '{date_col}'\")\n",
    "\n",
    "    # получаем максимальную дату в целевой таблице\n",
    "    max_date = conn.scalar(\n",
    "        text(\n",
    "            f'SELECT max(\"{date_col}\") FROM \"{SCHEMA}\".\"{table}\"'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df_new = df[df[date_col] > max_date] if max_date else df\n",
    "    if df_new.empty:\n",
    "        return 0\n",
    "\n",
    "    df_new.to_sql(\n",
    "        table,\n",
    "        conn,\n",
    "        schema=SCHEMA,\n",
    "        if_exists=\"append\",\n",
    "        index=False,\n",
    "        chunksize=CHUNK_SIZE,\n",
    "        method=\"multi\",\n",
    "    )\n",
    "    return len(df_new)\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# Example usage\n",
    "# ─────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    tasks: Dict[str, Dict] = {\n",
    "        \"ext_project_sync_designers\": {\n",
    "            \"df\": df_sync_designers,\n",
    "            \"date_col\": \"date\",\n",
    "        },\n",
    "        \"ext_project_sync_bim\": {\n",
    "            \"df\": df_sync_bim,\n",
    "            \"date_col\": \"date\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    with engine_postgres.begin() as conn:\n",
    "        db = conn.scalar(text(\"SELECT current_database()\"))\n",
    "        print(\"🔎 Подключен к базе:\", db)\n",
    "\n",
    "        for table, params in tasks.items():\n",
    "            added = _incremental_append(params[\"df\"], table, params[\"date_col\"], conn)\n",
    "            print(f\"✅ {table}: добавлено {added} новых строк\")\n",
    "            \n",
    "            \n",
    "# from sqlalchemy import text\n",
    "# import pandas as pd\n",
    "\n",
    "# # ✅ Проверка подключения к базе\n",
    "# with engine_postgres.begin() as conn:\n",
    "#     db_name = pd.read_sql(\"SELECT current_database()\", conn)\n",
    "#     print(\"🔎 Подключен к базе:\", db_name.iloc[0, 0])\n",
    "\n",
    "# # ✅ Пересоздание структуры таблицы\n",
    "# df_sync_designers.head(0).to_sql(\n",
    "#     \"ext_project_sync_designers\",\n",
    "#     engine_postgres,\n",
    "#     schema=\"datalake\",\n",
    "#     if_exists=\"replace\",  # Пересоздаёт таблицу с колонками из DataFrame\n",
    "#     index=False\n",
    "# )\n",
    "# print(\"🛠 Структура таблицы datalake.test_lake пересоздана из DataFrame.\")\n",
    "\n",
    "# # ✅ Загрузка данных в новую таблицу\n",
    "# df_sync_designers.to_sql(\n",
    "#     \"ext_project_sync_designers\",\n",
    "#     engine_postgres,\n",
    "#     schema=\"datalake\",\n",
    "#     if_exists=\"append\",\n",
    "#     index=False\n",
    "# )\n",
    "# print(f\"✅ Загружено строк: {len(df_sync_designers)}\")\n",
    "\n",
    "# # ✅ Пересоздание структуры таблицы\n",
    "# df_sync_bim.head(0).to_sql(\n",
    "#     \"ext_project_sync_bim\",\n",
    "#     engine_postgres,\n",
    "#     schema=\"datalake\",\n",
    "#     if_exists=\"replace\",  # Пересоздаёт таблицу с колонками из DataFrame\n",
    "#     index=False\n",
    "# )\n",
    "# print(\"🛠 Структура таблицы datalake.test_lake пересоздана из DataFrame.\")\n",
    "\n",
    "# # ✅ Загрузка данных в новую таблицу\n",
    "# df_sync_bim.to_sql(\n",
    "#     \"ext_project_sync_bim\",\n",
    "#     engine_postgres,\n",
    "#     schema=\"datalake\",\n",
    "#     if_exists=\"append\",\n",
    "#     index=False\n",
    "# )\n",
    "# print(f\"✅ Загружено строк: {len(df_sync_bim)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}