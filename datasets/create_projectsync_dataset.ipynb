{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3abbfb4",
   "metadata": {},
   "source": [
    "## Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ñ‹, Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ±Ğ¾Ñ€ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce6f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "BASE_DIR = pathlib.Path(os.environ['ETL_ROOT'])\n",
    "RAW_DIR = BASE_DIR / 'raw_data'\n",
    "PROC_DIR = BASE_DIR / 'processed'\n",
    "\n",
    "ad_path                        = RAW_DIR / 'tim_export_ad_user.csv'\n",
    "sync_path                      = RAW_DIR / 'tim_export_project_sync.csv'\n",
    "\n",
    "save_path = PROC_DIR / 'sync_transformed.csv'\n",
    "\n",
    "engine_postgres = create_engine(\"postgresql+psycopg2://postgres:Q!w2e3r4@192.168.42.188:5430/postgres\")\n",
    "engine_pluginsdb = create_engine(\"postgresql+psycopg2://postgres:Q!w2e3r4@192.168.42.188:5430/pluginsdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ca2d7a",
   "metadata": {},
   "source": [
    "## Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ñ‚Ğ°Ñ„Ñ€ĞµĞ¹Ğ¼Ğ¾Ğ² Ğ¿Ğ¾ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe1416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ad = pd.read_csv(ad_path)\n",
    "df_sync = pd.read_csv(sync_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37703e5",
   "metadata": {},
   "source": [
    "## Ğ¡Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f8460",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sync = df_sync.merge(\n",
    "    df_ad[[\"display_name\", \"department\", \"project_section\"]],\n",
    "    how=\"left\",\n",
    "    left_on=\"user_display_name\",\n",
    "    right_on=\"display_name\"\n",
    ").drop(columns=\"display_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1225895f",
   "metadata": {},
   "source": [
    "## Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ»Ğ¸ÑˆĞ½Ğ¸Ñ… ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ğ¾Ğ² Ğ² Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f67118",
   "metadata": {},
   "outputs": [],
   "source": [
    "bim_users = {\n",
    "    'ĞšĞ¾Ğ»Ğ¿Ğ°ĞºĞ¾Ğ² Ğ¡ĞµĞ¼ĞµĞ½ Ğ”Ğ¼Ğ¸Ñ‚Ñ€Ğ¸ĞµĞ²Ğ¸Ñ‡','ĞŸÑÑ‚ĞºĞ¾Ğ² Ğ Ğ¾Ğ¼Ğ°Ğ½ ĞĞ½Ğ°Ñ‚Ğ¾Ğ»ÑŒĞµĞ²Ğ¸Ñ‡',\n",
    "    'ĞĞ½Ğ´Ñ€ĞµĞµĞ² ĞĞ»ĞµĞºÑĞ°Ğ½Ğ´Ñ€ ĞšĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ğ¸Ğ½Ğ¾Ğ²Ğ¸Ñ‡','ĞšĞ¸Ñ‡Ğ¸Ğ³Ğ¸Ğ½ ĞĞ½Ğ´Ñ€ĞµĞ¹ Ğ’Ğ»Ğ°Ğ´Ğ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ¸Ñ‡',\n",
    "    'ĞŸĞ°Ğ½Ğ¾Ğ² ĞĞ½Ñ‚Ğ¾Ğ½ Ğ’Ğ»Ğ°Ğ´Ğ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ¸Ñ‡','Ğ’Ğ°ÑÑŒĞºĞ¾Ğ² Ğ”ĞµĞ½Ğ¸Ñ Ğ˜Ğ³Ğ¾Ñ€ĞµĞ²Ğ¸Ñ‡','ĞŸĞ¾Ğ¿Ğ¾Ğ² ĞĞ½Ñ‚Ğ¾Ğ½ ĞœĞ¸Ñ…Ğ°Ğ¹Ğ»Ğ¾Ğ²Ğ¸Ñ‡',\n",
    "    'ĞšÑƒĞ·Ğ¾Ğ²Ğ»ĞµĞ²Ğ° ĞĞ»ÑŒĞ³Ğ° Ğ¡ĞµÑ€Ğ³ĞµĞµĞ²Ğ½Ğ°','ĞšĞ°Ğ»Ğ°Ñ‡ĞµĞ² Ğ”Ğ°Ğ½Ğ¸Ğ¸Ğ» ĞÑ€Ñ‚ĞµĞ¼Ğ¾Ğ²Ğ¸Ñ‡',\n",
    "    'Ğ“Ñ€Ğ¸Ğ³Ğ¾Ñ€ÑŒĞµĞ² Ğ Ğ¾Ğ¼Ğ°Ğ½ ĞĞ¸ĞºĞ¾Ğ»Ğ°ĞµĞ²Ğ¸Ñ‡','ĞšÑ€Ğ°ÑĞ¸Ğ»ÑŒĞ½Ğ¸ĞºĞ¾Ğ² Ğ”Ğ¼Ğ¸Ñ‚Ñ€Ğ¸Ğ¹ Ğ¡ĞµÑ€Ğ³ĞµĞµĞ²Ğ¸Ñ‡',\n",
    "    'Ğ›Ğ¸Ñ‚ÑƒĞµĞ²Ğ° Ğ®Ğ»Ğ¸Ñ Ğ”Ğ¼Ğ¸Ñ‚Ñ€Ğ¸ĞµĞ²Ğ½Ğ°','Ğ–ÑƒĞº Ğ’Ğ¸Ñ‚Ğ°Ğ»Ğ¸Ğ¹ Ğ¢Ğ¾Ğ¼Ğ°ÑˆĞµĞ²Ğ¸Ñ‡','ĞĞ²ÑÑĞ½ĞºĞ¸Ğ½ Ğ Ğ¾Ğ¼Ğ°Ğ½ ĞĞ¸ĞºĞ¾Ğ»Ğ°ĞµĞ²Ğ¸Ñ‡',\n",
    "    'Ğ Ğ¾Ğ¼Ğ°Ğ½Ğ¾Ğ²Ğ° ĞĞ½Ğ½Ğ° Ğ’ÑÑ‡ĞµÑĞ»Ğ°Ğ²Ğ¾Ğ²Ğ½Ğ°','ĞšĞ¾Ğ½Ğ¾Ğ²Ğ°Ğ»Ğ¾Ğ² Ğ’Ğ°ÑĞ¸Ğ»Ğ¸Ğ¹ Ğ¡ĞµÑ€Ğ³ĞµĞµĞ²Ğ¸Ñ‡',\n",
    "    'Ğ£Ñ€Ğ¼Ğ°Ğ½Ñ‡ĞµĞµĞ² Ğ Ğ¾Ğ¼Ğ°Ğ½ Ğ”Ğ°Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ¸Ñ‡', 'Ğ”Ğ¾ĞºĞ»Ğ°Ğ´Ñ‡Ğ¸Ğº 708'\n",
    "}\n",
    "\n",
    "df_sync[\"is_bim\"] = df_sync[\"user_display_name\"].isin(bim_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cce841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_short_name(name: str) -> str:\n",
    "    parts = name.split('_')\n",
    "    return '_'.join(parts[:2]) if len(parts) >= 2 else name\n",
    "\n",
    "df_sync['short_project_name'] = df_sync['project_name'].astype(str).apply(extract_short_name)\n",
    "\n",
    "df_sync = df_sync.drop(columns=[\n",
    "    'program_name',\n",
    "    'program_version',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b6aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_atom = df_sync[\"project_name\"].str.contains(\n",
    "    \"ĞĞ¢ĞĞœ|Ğ”ĞĞ£|08-12|Ğ˜ĞšĞŸ|ATOM|ĞĞŸĞ£\", case=False, na=False\n",
    ")\n",
    "\n",
    "df_sync[\"object_name\"] = np.select(\n",
    "    [\n",
    "        df_sync[\"project_name\"].str.contains(\"Ğ¡ĞŸ.Ğ›Ğ›Ğ£|ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚|ÑƒĞ·Ğ»Ñ‹|ÑƒĞ·ĞµĞ»|Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ°\", case=False, na=False),\n",
    "        mask_atom,\n",
    "        df_sync[\"project_name\"].str.contains(\"K01\", case=False, na=False),\n",
    "        df_sync[\"project_name\"].str.contains(\"Ğ˜ĞĞŸĞ Ğ\", case=False, na=False),\n",
    "        df_sync[\"project_name\"].str.contains(\"Ğ¯Ğ»Ñ‚Ğ°\", case=False, na=False)\n",
    "    ],\n",
    "    [\n",
    "        \"Ğ£Ğ·Ğ»Ñ‹ Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ñ‹\",\n",
    "        \"ĞĞ¢ĞĞœ\",\n",
    "        \"ĞšĞ¾Ñ€Ñ‚Ñ€Ğ¾Ñ\",\n",
    "        \"Ğ˜ĞĞŸĞ Ğ\",\n",
    "        \"Ğ¯Ğ»Ñ‚Ğ°\"\n",
    "    ],\n",
    "    default=\"ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ñ‹\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc9ebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sync[\"is_detached\"] = df_sync[\"project_name\"].str.contains(\"Ğ¾Ñ‚ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¾\", case=False, na=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce27465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file_storage_name(row):\n",
    "    project = row.get(\"project_name\")\n",
    "    username = row.get(\"username\")\n",
    "\n",
    "    if pd.isna(project) or pd.isna(username):\n",
    "        return project  # Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ ĞºĞ°Ğº ĞµÑÑ‚ÑŒ\n",
    "\n",
    "    parts = str(project).split(\"_\")\n",
    "    if len(parts) < 2:\n",
    "        return project  # Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ ĞºĞ°Ğº ĞµÑÑ‚ÑŒ\n",
    "\n",
    "    last_part = parts[-1].strip().lower()\n",
    "    user_name = str(username).strip().lower()\n",
    "\n",
    "    if last_part == user_name:\n",
    "        return \"_\".join(parts[:-1])\n",
    "    else:\n",
    "        return project  # Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ ĞºĞ°Ğº ĞµÑÑ‚ÑŒ\n",
    "\n",
    "df_sync[\"file_storage_name\"] = df_sync.apply(extract_file_storage_name, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c11cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_project_solution(row):\n",
    "    name = str(row[\"project_name\"])\n",
    "    obj  = row[\"object_name\"]\n",
    "\n",
    "    if obj == \"ĞšĞ¾Ñ€Ñ‚Ñ€Ğ¾Ñ\":\n",
    "        section_map_kortros = {\n",
    "            \"_AR\": \"ĞĞ \", \"_AI\": \"ĞĞ˜\",\n",
    "            \"_KR\": \"ĞšĞ \", \"_AGK\": \"ĞĞ“Ğš\",\n",
    "            \"_VK\": \"Ğ’Ğš\", \"_EL\": \"Ğ­Ğ›\",\n",
    "            \"_OV\": \"ĞĞ’\", \"_AK\": \"ĞĞš\",\n",
    "            \"_SS\": \"Ğ¡Ğ¡\", \"_P\": \"ĞŸ\",\n",
    "            \"_R\": \"Ğ \", \"_TS\": \"Ğ¢Ğ¡\",\n",
    "            \"_AP\": \"ĞĞŸ\"\n",
    "        }\n",
    "\n",
    "        for pattern, section in section_map_kortros.items():\n",
    "            if pattern in name:\n",
    "                return section\n",
    "        return \"ĞĞ”\"\n",
    "\n",
    "    else:\n",
    "        section_map_rus = {\n",
    "            \"_ĞĞ \": \"ĞĞ \", \"_Ğ¤Ğ¾Ñ€ÑÑĞºĞ¸Ğ·\": \"ĞĞ \",\n",
    "            \"_ĞĞ˜\": \"ĞĞ˜\", \"_ĞšĞ–\": \"ĞšĞ–\",\n",
    "            \"_Ğ’Ğš\": \"Ğ’Ğš\", \"_Ğ­Ğ›\": \"Ğ­Ğ›\",\n",
    "            \"_Ğ¢Ğ¡\": \"Ğ¢Ğ¡\", \"_Ğ¢Ğ¥\": \"Ğ¢Ğ¥\",\n",
    "            \"_ĞĞ’\": \"ĞĞ’\", \"_ĞšĞ \": \"ĞšĞ \",\n",
    "            \"_ĞšĞœ\": \"ĞšĞœ\", \"_ĞĞŸ\": \"ĞĞŸ\",\n",
    "            \"_ĞŸĞ¢\": \"ĞŸĞ¢\", \"_Ğ¡Ğ¡\": \"Ğ¡Ğ¡\",\n",
    "            \"_ĞŸĞ‘\": \"ĞŸĞ‘\", \"_Ğ­Ğ“\": \"Ğ­Ğ“\",\n",
    "             \"_ĞĞŸ\": \"ĞĞŸ\"\n",
    "        }\n",
    "\n",
    "        for pattern, section in section_map_rus.items():\n",
    "            if pattern in name:\n",
    "                return section\n",
    "        return \"ĞĞ”\"\n",
    "\n",
    "df_sync[\"project_solution_name\"] = df_sync.apply(get_project_solution, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b945d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_project_stage(row):\n",
    "    name = str(row[\"project_name\"])\n",
    "    obj = row[\"object_name\"]\n",
    "\n",
    "    if obj == \"ĞšĞ¾Ñ€Ñ‚Ñ€Ğ¾Ñ\":\n",
    "        # ĞºĞ¾Ñ€Ñ‚ĞµĞ¶: (Ñ‚Ğ¸Ğ¿_Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸, Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½): Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ\n",
    "        stage_map_kortros = {\n",
    "            (\"contains\", \"_P_\"): \"ĞŸ\",\n",
    "            (\"contains\", \"_R_\"): \"Ğ \",\n",
    "            (\"contains\", \"_AGK_\"): \"Ğ“Ğš\",\n",
    "            (\"endswith\", \"_P\"): \"ĞŸ\",\n",
    "            (\"endswith\", \"_R\"): \"Ğ \",\n",
    "            (\"endswith\", \"_AGK\"): \"Ğ“Ğš\"\n",
    "        }\n",
    "\n",
    "        for (mode, pattern), stage in stage_map_kortros.items():\n",
    "            if (mode == \"contains\" and pattern in name) or \\\n",
    "               (mode == \"endswith\" and name.endswith(pattern)):\n",
    "                return stage\n",
    "\n",
    "        return \"ĞĞ”\"\n",
    "\n",
    "    else:\n",
    "        stage_map = {\n",
    "            (\"contains\", \"_ĞŸ_\"): \"ĞŸ\",\n",
    "            (\"contains\", \"_Ğ _\"): \"Ğ \",\n",
    "            (\"contains\", \"_Ğ Ğ”_\"): \"Ğ \",\n",
    "            (\"contains\", \"_Ğ­ĞŸ_\"): \"Ğ­ĞŸ\",\n",
    "            (\"contains\", \"_Ğ¤Ğ¾Ñ€ÑÑĞºĞ¸Ğ·_\"): \"Ğ­ĞŸ\",\n",
    "            (\"contains\", \"_Ğ­ÑĞºĞ¸Ğ·_\"): \"Ğ­ĞŸ\",\n",
    "            (\"contains\", \"_Ğ¤Ğ­_\"): \"Ğ­ĞŸ\",\n",
    "            (\"endswith\", \"_ĞŸ\"): \"ĞŸ\",\n",
    "            (\"endswith\", \"_Ğ \"): \"Ğ \",\n",
    "            (\"endswith\", \"_Ğ Ğ”\"): \"Ğ \",\n",
    "            (\"endswith\", \"_Ğ­ĞŸ\"): \"Ğ­ĞŸ\",\n",
    "            (\"endswith\", \"_Ğ¤Ğ¾Ñ€ÑÑĞºĞ¸Ğ·\"): \"Ğ­ĞŸ\",\n",
    "            (\"endswith\", \"_Ğ­ÑĞºĞ¸Ğ·\"): \"Ğ­ĞŸ\",\n",
    "            (\"endswith\", \"_Ğ¤Ğ­\"): \"Ğ­ĞŸ\"\n",
    "        }\n",
    "\n",
    "        for (mode, pattern), stage in stage_map.items():\n",
    "            if (mode == \"contains\" and pattern in name) or \\\n",
    "               (mode == \"endswith\" and name.endswith(pattern)):\n",
    "                return stage\n",
    "\n",
    "        return \"ĞĞ”\"\n",
    "    \n",
    "df_sync[\"project_stage_name\"] = df_sync.apply(get_project_stage, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a683aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_cols = df_sync.select_dtypes(include='object').columns\n",
    "df_sync[str_cols] = df_sync[str_cols].fillna(\"ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…\")\n",
    "\n",
    "num_cols = df_sync.select_dtypes(include=['number', 'Int64']).columns\n",
    "df_sync[num_cols] = df_sync[num_cols].fillna(0)\n",
    "\n",
    "date_cols = df_sync.select_dtypes(include='datetime').columns\n",
    "df_sync[date_cols] = df_sync[date_cols].fillna(pd.NaT)  # Ğ˜Ğ»Ğ¸ Ğ·Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a61cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sync_bim = df_sync[(df_sync['is_bim'] == True) & (df_sync['is_detached'] == 0)].copy()\n",
    "df_sync_designers = df_sync[(df_sync['is_bim'] == False) & (df_sync['is_detached'] == 0)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beec76a9",
   "metadata": {},
   "source": [
    "## ĞŸĞ¸ÑˆĞµĞ¼ Ğ² Ğ‘Ğ”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddca4c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Ğ˜Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° DataFrameâ€‘Ğ¾Ğ² Ğ² PostgreSQL.\n",
    "\n",
    "**ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½ ĞºĞ»ÑÑ‡â€‘Ğ´Ğ°Ñ‚Ğ°, Ğ° Ğ½Ğµ GUID?**\n",
    "-----------------------------------------------------------------\n",
    "* Ğ’ Ğ²Ğ°ÑˆĞ¸Ñ… Ğ²Ñ‹Ğ³Ñ€ÑƒĞ·ĞºĞ°Ñ… ÑÑ‚Ñ€Ğ¾ĞºĞ¸ Ğ¿Ğ¾ÑĞ²Ğ»ÑÑÑ‚ÑÑ *Ğ¾Ğ´Ğ¸Ğ½ Ñ€Ğ°Ğ·* Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ½Ğµ Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ, Ğ°Â Ğ½Ğ¾Ğ²Ñ‹Ğµ â€” Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾Ğ»ĞµĞ¼â€‘Ğ´Ğ°Ñ‚Ñ‹ (snapshot / update_date / loaded_at).\n",
    "* ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ°Ñ‚Ğ° Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ÑÑ Ğ·Ğ° O(1) Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ Ğ¸Ğ½Ğ´ĞµĞºÑĞ° Ğ¸ ÑÑ€Ğ°Ğ·Ñƒ Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚, ĞºĞ°ĞºĞ¸Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ.\n",
    "* GUID, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ»ĞµÑ‚Ñƒ (`uuid4()`), Ğ½Ğµ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½: Ğ¿Ñ€Ğ¸ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¼ Ğ·Ğ°Ğ¿ÑƒÑĞºĞµ ÑÑ‚Ğ°Ñ€Ñ‹Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ GUIDâ€‘Ñ‹ Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‚ Ğ²ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾.\n",
    "* Ğ”ĞµÑ‚â€‘GUID Ğ¸Ğ· Ñ…ÑÑˆĞ° ÑÑ‚Ñ€Ğ¾ĞºĞ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²ÑÑ‘â€‘Ñ€Ğ°Ğ²Ğ½Ğ¾ Ğ´ÑƒĞ±Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ°Ñ‚Ñ‹.\n",
    "\n",
    "Ğ˜Ñ‚Ğ¾Ğ³Ğ¾: **ĞºĞ»ÑÑ‡â€‘Ğ´Ğ°Ñ‚Ğ° Ğ¿Ñ€Ğ¾Ñ‰Ğµ, Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¸ Ğ½Ğµ Ğ¼ĞµĞ½ÑĞµÑ‚ ÑÑ…ĞµĞ¼Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…**.\n",
    "\n",
    "----------------------------------------------------------------------------------------------------\n",
    "Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚\n",
    "====================================================================\n",
    "Ğ—Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸: pandas, numpy, SQLAlchemyÂ â‰¥1.4, psycopg2â€‘binary.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, inspect, text\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SCHEMA = \"datalake\"\n",
    "CHUNK_SIZE = 5_000\n",
    "\n",
    "def _ensure_table_and_columns(df: pd.DataFrame, table: str, conn) -> None:\n",
    "    \"\"\"Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ Ğ¸Ğ»Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°ÑÑ‰Ğ¸Ğµ ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ñ‹.\"\"\"\n",
    "    insp = inspect(conn)\n",
    "    if not insp.has_table(table, schema=SCHEMA):\n",
    "        df.head(0).to_sql(table, conn, schema=SCHEMA, if_exists=\"replace\", index=False)\n",
    "        return\n",
    "\n",
    "    existing_cols = {c[\"name\"] for c in insp.get_columns(table, schema=SCHEMA)}\n",
    "    missing_cols = [c for c in df.columns if c not in existing_cols]\n",
    "    for col in missing_cols:\n",
    "        sql_type = str(df[col].dtype)\n",
    "        # ÑƒĞ¿Ñ€Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾: pandas Ğ¿Ñ€Ğ¸Ğ²ĞµĞ´Ñ‘Ñ‚ Ğº text/varchar; Ğ´Ğ»Ñ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾Ğ¹ Ñ‚Ğ¸Ğ¿Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾\n",
    "        # Ğ²Ñ‹Ğ·Ğ²Ğ°Ñ‚ÑŒ pandas.io.sql.get_sqltype ĞºĞ°Ğº Ğ² Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ¹ Ğ²ĞµÑ€ÑĞ¸Ğ¸.\n",
    "        conn.execute(\n",
    "            text(\n",
    "                f'ALTER TABLE \"{SCHEMA}\".\"{table}\" '\n",
    "                f'ADD COLUMN IF NOT EXISTS \"{col}\" {sql_type}'\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def _incremental_append(\n",
    "    df: pd.DataFrame,\n",
    "    table: str,\n",
    "    date_col: str,\n",
    "    conn,\n",
    ") -> int:\n",
    "    \"\"\"Ğ’ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ñ€Ğ¾ĞºĞ¸, Ñƒ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… `date_col` > max(date_col) Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğµ.\"\"\"\n",
    "    _ensure_table_and_columns(df, table, conn)\n",
    "\n",
    "    # ĞµÑĞ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°â€‘ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸ Ğ½ĞµÑ‚ â€” Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ\n",
    "    if date_col not in df.columns:\n",
    "        raise KeyError(f\"Ğ’ DataFrame Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ° Ñ Ğ´Ğ°Ñ‚Ğ¾Ğ¹ '{date_col}'\")\n",
    "\n",
    "    # Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ´Ğ°Ñ‚Ñƒ Ğ² Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğµ\n",
    "    max_date = conn.scalar(\n",
    "        text(\n",
    "            f'SELECT max(\"{date_col}\") FROM \"{SCHEMA}\".\"{table}\"'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df_new = df[df[date_col] > max_date] if max_date else df\n",
    "    if df_new.empty:\n",
    "        return 0\n",
    "\n",
    "    df_new.to_sql(\n",
    "        table,\n",
    "        conn,\n",
    "        schema=SCHEMA,\n",
    "        if_exists=\"append\",\n",
    "        index=False,\n",
    "        chunksize=CHUNK_SIZE,\n",
    "        method=\"multi\",\n",
    "    )\n",
    "    return len(df_new)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Example usage\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    tasks: Dict[str, Dict] = {\n",
    "        \"ext_project_sync_designers\": {\n",
    "            \"df\": df_sync_designers,\n",
    "            \"date_col\": \"date\",\n",
    "        },\n",
    "        \"ext_project_sync_bim\": {\n",
    "            \"df\": df_sync_bim,\n",
    "            \"date_col\": \"date\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    with engine_postgres.begin() as conn:\n",
    "        db = conn.scalar(text(\"SELECT current_database()\"))\n",
    "        print(\"ğŸ” ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½ Ğº Ğ±Ğ°Ğ·Ğµ:\", db)\n",
    "\n",
    "        for table, params in tasks.items():\n",
    "            added = _incremental_append(params[\"df\"], table, params[\"date_col\"], conn)\n",
    "            print(f\"âœ… {table}: Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ {added} Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ñ€Ğ¾Ğº\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
